{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3312f04d",
   "metadata": {},
   "source": [
    "# Mellea Complete Tutorial\n",
    "\n",
    "Welcome to the comprehensive interactive tutorial for **Mellea**.\n",
    "This notebook combines practical usage patterns with core architectural concepts.\n",
    "\n",
    "**Docs Reference:** [Welcome](https://docs.mellea.ai/overview/mellea-welcome), [Quickstart](https://docs.mellea.ai/overview/quick-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a49752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anupama/2026/AI/mellea/mellea-IBM/mellea_venv/lib/python3.14/site-packages/granite_common/granite3/granite33/output.py:184: SyntaxWarning: \"\\d\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\d\"? A raw string is also an option.\n",
      "  f'{re.escape(CITE_START)}{{\"document_id\": \"(\\d+)\"}}{re.escape(CITE_END)}'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Mellea Session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pulling 6c02683809a8 sha256:6c026: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 2.10G/2.10G [03:20<00:00, 10.6MB/s]\n",
      "pulling 0f6ec9740c76 sha256:0f6ec:   0%|                                | 0.00/7.08k [00:00<?, ?B/s]\u001b[A\n",
      "pulling 0f6ec9740c76 sha256:0f6ec: 100%|███████████████████████| 7.08k/7.08k [00:00<00:00, 11.9kB/s]\u001b[A\n",
      "\n",
      "pulling cfc7749b96f6 sha256:cfc77:   0%|                                | 0.00/11.4k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "pulling cfc7749b96f6 sha256:cfc77: 100%|███████████████████████| 11.4k/11.4k [00:00<00:00, 19.1kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "pulling ba32b08db168 sha256:ba32b:   0%|                                  | 0.00/417 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                                                                                                                                                            \n",
      "                                                                                                    \u001b[A\n",
      "\n",
      "                                                                                                    \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                                    \u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session started! Model: granite4:micro\n"
     ]
    }
   ],
   "source": [
    "import mellea\n",
    "from mellea.backends import ModelOption\n",
    "from mellea.backends.ollama import OllamaModelBackend\n",
    "\n",
    "# 1. Initialize Mellea with the local Ollama backend and Granite model\n",
    "# We use a fixed seed for reproducibility in this tutorial.\n",
    "print(\"Initializing Mellea Session...\")\n",
    "m = mellea.MelleaSession(\n",
    "    backend=OllamaModelBackend(\n",
    "        model_id=\"granite4:micro\", \n",
    "        model_options={ModelOption.SEED: 42} # Fixed seed for reproducibility\n",
    "    )\n",
    ")\n",
    "print(\"Session started! Model: granite4:micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a633fe",
   "metadata": {},
   "source": [
    "## Part 1: Basic Instruction\n",
    "\n",
    "The core of Mellea is the `instruct` method. You give it a natural language instruction, and it returns a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66cee739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 11:49:03-INFO ======\n",
      "SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         | 0/2 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code in Python flows,\n",
      "Elegance meets simplicity.\n",
      "Logic takes its stage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response = m.instruct(\"Write a short haiku about writing python code.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5576fdbc",
   "metadata": {},
   "source": [
    "## Part 2: Components & CBlocks (Core Concepts)\n",
    "\n",
    "Most LLM libraries just pass strings around. Mellea is different: it uses **Components**.\n",
    "\n",
    "A `Component` is a structured object Mellea can format for an LLM call.\n",
    "\n",
    "The minimum you need to define a custom component is:\n",
    "- `parts()` → list of sub-parts (other Components or CBlocks)\n",
    "- `format_for_llm()` → string (or TemplateRepresentation)\n",
    "\n",
    "Then you can run it with `m.act(component)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f381d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Flashcards...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 11:57:35-INFO ======\n",
      "SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/2 [00:21<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw model output: {\n",
      "  \"topic\": \"IBM Quantum hardware\",\n",
      "  \"cards\": [\n",
      "    {\n",
      "      \"question\": \"What is the primary function of IBM's quantum hardware?\",\n",
      "      \"answer\": \"The primary function of IBM's quantum hardware is to perform computations using quantum bits (qubits) and leverage principles such as superposition and entanglement for complex problem-solving.\",\n",
      "      \"difficulty\": \"easy\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Which company developed the first commercially available quantum computer?\",\n",
      "      \"answer\": \"IBM developed the first commercially available quantum computer, known as IBM Q System One.\",\n",
      "      \"difficulty\": \"medium\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What is a qubit in IBM Quantum hardware?\",\n",
      "      \"answer\": \"A qubit (quantum bit) is the fundamental unit of information in quantum computing. Unlike classical bits which can be either 0 or 1, a qubit can exist in multiple states simultaneously due to superposition.\",\n",
      "      \"difficulty\": \"hard\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Parsed Object: topic='IBM Quantum hardware' cards=[{'question': \"What is the primary function of IBM's quantum hardware?\", 'answer': \"The primary function of IBM's quantum hardware is to perform computations using quantum bits (qubits) and leverage principles such as superposition and entanglement for complex problem-solving.\", 'difficulty': 'easy'}, {'question': 'Which company developed the first commercially available quantum computer?', 'answer': 'IBM developed the first commercially available quantum computer, known as IBM Q System One.', 'difficulty': 'medium'}, {'question': 'What is a qubit in IBM Quantum hardware?', 'answer': 'A qubit (quantum bit) is the fundamental unit of information in quantum computing. Unlike classical bits which can be either 0 or 1, a qubit can exist in multiple states simultaneously due to superposition.', 'difficulty': 'hard'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mellea.core import Component, CBlock\n",
    "import pydantic\n",
    "\n",
    "# Define a structured output format using Pydantic\n",
    "class FlashcardSetOut(pydantic.BaseModel):\n",
    "    topic: str\n",
    "    cards: list[dict]\n",
    "\n",
    "# Define a custom Component\n",
    "class FlashcardMaker(Component):\n",
    "    def __init__(self, topic: str, count: int = 5):\n",
    "        self.topic = topic\n",
    "        self.count = count\n",
    "\n",
    "    def parts(self):\n",
    "        # We don't have nested parts in this simple example.\n",
    "        return []\n",
    "\n",
    "    def format_for_llm(self):\n",
    "        # Returning a STRING is the simplest valid way to represent a Component.\n",
    "        return (\n",
    "            \"You are a study assistant.\\n\"\n",
    "            f\"Make {self.count} flashcards about: {self.topic}.\\n\"\n",
    "            \"Return JSON with keys: topic (string), cards (list of objects).\\n\"\n",
    "            \"Each card object must have: question, answer, difficulty.\\n\"\n",
    "        )\n",
    "\n",
    "    def _parse(self, computed):\n",
    "        # In a real component, you might parse the JSON here.\n",
    "        # For this tutorial, we return the raw value and parse it manually outside.\n",
    "        return computed.value\n",
    "\n",
    "# Act on the component\n",
    "print(\"Generating Flashcards...\")\n",
    "res = m.act(FlashcardMaker(\"IBM Quantum hardware\", count=3), format=FlashcardSetOut)\n",
    "\n",
    "print(\"\\nRaw model output:\", res.value)\n",
    "\n",
    "# Parse it back into the Pydantic model\n",
    "parsed = FlashcardSetOut.model_validate_json(res.value)\n",
    "print(\"\\nParsed Object:\", parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc91114",
   "metadata": {},
   "source": [
    "# The Loop: Instruct-Validate-Repair (Docs: Requirements)\n",
    "\n",
    "Generative models can be unpredictable. Mellea's power lies in its **Instruct-Validate-Repair** loop. \n",
    "You can define **Requirements** (`req`) and **Checks** (`check`) that the output *must* satisfy. If the model fails, Mellea uses a **Strategy** to try again or fix it.\n",
    "\n",
    "**Docs Reference:** [Requirements](https://docs.mellea.ai/overview/requirements)\n",
    "\n",
    "### Example: Generating a Helper Email with Constraints\n",
    "We want to generate an email that:\n",
    "1.  Has a salutation.\n",
    "2.  Does **not** use the word 'regards'.\n",
    "3.  (Optional strict check) Uses only lower-case letters (just to show validation failure and repair)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f49ef0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 11:57:56-INFO ======\n",
      "Starting Mellea session: backend=ollama, model=granite4:micro, context=SimpleContext\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 11:57:58-INFO ======\n",
      "SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/5 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: acknowledging your excellent teamwork\n",
      "\n",
      "dear anupama,\n",
      "\n",
      "i hope this message finds you well. i am writing to highlight how impressed we are with your contributions and dedication as part of our team.\n",
      "\n",
      "your commitment to being a great teammate is truly remarkable, especially your eagerness to assist others and contribute significantly to group projects. these qualities not only foster a collaborative environment but also greatly enhance the overall productivity and morale within the team.\n",
      "\n",
      "keep up the fantastic work, anupama!\n",
      "\n",
      "warm regards,\n",
      "\n",
      "[your name]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mellea.stdlib.sampling import RejectionSamplingStrategy\n",
    "from mellea.stdlib.requirements import req,check,simple_validate\n",
    "\n",
    "# using a validation fn to validate your results as well as a check to validate your results\n",
    "requirements = [\n",
    "    req(\"The email should have a salutation\"),\n",
    "    req(\"Use only lower-case letters\", validation_fn=simple_validate(lambda s: s.islower())),\n",
    "    check(\"The email should not mention the word 'regards'\")\n",
    "        \n",
    "]\n",
    "\n",
    "def write_email(m:mellea.MelleaSession, names :str , notes:str)->str:\n",
    "    email_candidate = m.instruct(\n",
    "        \"Write an email to {{name}} using the notes following: {{notes}}.\",\n",
    "        requirements=requirements,\n",
    "        strategy = RejectionSamplingStrategy(loop_budget = 5),\n",
    "        user_variables={\"name\": names, \"notes\": notes},\n",
    "        return_sampling_results = True ,\n",
    "    )\n",
    "    if email_candidate.success :\n",
    "        return str(email_candidate.result)\n",
    "    else :\n",
    "        print(\"Expect sub-par output due to failure to meet requirements.\")\n",
    "        return email_candidate.sampling_results[0].value\n",
    "    \n",
    "m=mellea.start_session()\n",
    "print(write_email(m,\"Anupama\",\"Anupama has been a great team player, always willing to help others and contribute to group projects.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed572285",
   "metadata": {},
   "source": [
    "# Advanced: Core Concepts & Generative Programming\n",
    "\n",
    "**Docs Reference:** [Generative Programming](https://docs.mellea.ai/overview/project-mellea), [Core Concepts](https://docs.mellea.ai/core-concept/generative-slots)\n",
    "\n",
    "In this section, we touch on more advanced concepts. Mellea treats prompts not just as strings, but as programs with **Slots**, **Context**, and **Agents**.\n",
    "\n",
    "### Variable Injection (Context)\n",
    "You saw `user_variables` above. This is part of Mellea's Context Management system, allowing you to inject data safely into prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "011c4e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 11:56:48-INFO ======\n",
      "SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/2 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello in Spanish: The translation of \"Hello\" in Spanish is \"Hola\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick example of dynamic variables\n",
    "response = m.instruct(\n",
    "    \"Translate the following word to Spanish: {{word}}\",\n",
    "    user_variables={\"word\": \"Hello\"}\n",
    ")\n",
    "print(f\"Hello in Spanish: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a78c7ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 16:16:08-INFO ======\n",
      "SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mathematical expression \"2x2\" represents the multiplication of two numbers: 2 and 2. Therefore, 2 multiplied by 2 equals 4. In other words, if you have two groups with two items in each group, the total number of items would be four.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# using model options\n",
    "import mellea\n",
    "from mellea.backends import ModelOption  #its mellea.backends\n",
    "from mellea.backends.ollama import OllamaModelBackend\n",
    "from mellea.backends import model_ids\n",
    "\n",
    "m = mellea.MelleaSession(\n",
    "    backend=OllamaModelBackend(\n",
    "        model_id=\"granite4:micro\",model_options={ModelOption.SEED: 42}\n",
    "    )\n",
    ")\n",
    "\n",
    "answer = m.instruct(\n",
    "    \"What is 2x2?\",\n",
    "    model_options={\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(str(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae56a9",
   "metadata": {},
   "source": [
    "## Generative Slots\n",
    "\n",
    "``GenerativeSlot`` is a function whose implementation is provieded by an LLM.In Mellea you define these using ``@generative`` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e5b967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 16:16:08-INFO ======\n",
      "Starting Mellea session: backend=ollama, model=granite4:micro, context=ChatContext, model_options={'@@@max_new_tokens@@@': 10}\u001b[0m\n",
      "grade =  10\n",
      "grade =  2\n",
      "grade =  8\n"
     ]
    }
   ],
   "source": [
    "# from mellea.std.base import ChatContext is old \n",
    "from mellea.stdlib.context import ChatContext\n",
    "from mellea import start_session\n",
    "from mellea.backends import ModelOption\n",
    "from mellea import generative   \n",
    "\n",
    "# ChatContext is used to maintain conversation history across multiple model calls. \n",
    "# Unlike SimpleContext (the default), which resets the chat history on each call, \n",
    "# ChatContext behaves like a chat history where previous messages are remembered.\n",
    "\n",
    "@generative\n",
    "def grade_syntax(code: str) -> int:\n",
    "    \"\"\" Grade the code based on correct implementation of the function\n",
    "    args:\n",
    "        code: str (to be graded)\n",
    "    returns:\n",
    "        int : a grade between 1(worst) and 10(best)\n",
    "    \"\"\"\n",
    "codes = (\n",
    "    \" def add(a, b):\\n    return a + b\",\n",
    "    \"def subtract(a,b): cout<<a-b<<endl\",\n",
    "    \"int multiply(int a,int b) { return a*b}\"\n",
    ")\n",
    "m = start_session(ctx = ChatContext() , model_options = {ModelOption.MAX_NEW_TOKENS: 10})\n",
    "for code in codes :\n",
    "    grade = grade_syntax(m, code = code)\n",
    "    print(\"grade = \",grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b260f3f",
   "metadata": {},
   "source": [
    "### Advanced: Using Generative slots to Provide Compositionality Across Module Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b61b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: https://github.com/generative-computing/mellea/blob/main/docs/examples/tutorial/compositionality_with_generative_slots.py#L1-L18\n",
    "from mellea import generative\n",
    "from typing import Literal\n",
    "\n",
    "# The Summarizer Library\n",
    "@generative\n",
    "def summarize_meeting(transcript: str) -> str:\n",
    "  \"\"\"Summarize the meeting transcript into a concise paragraph of main points.\"\"\"\n",
    "\n",
    "@generative\n",
    "def summarize_contract(contract_text: str) -> str:\n",
    "  \"\"\"Produce a natural language summary of contract obligations and risks.\"\"\"\n",
    "\n",
    "@generative\n",
    "def summarize_short_story(story: str) -> str:\n",
    "  \"\"\"Summarize a short story, with one paragraph on plot and one paragraph on broad themes.\"\"\"\n",
    "\n",
    "\n",
    "# The Decision Aides Library\n",
    "@generative\n",
    "def propose_business_decision(summary: str) -> str:\n",
    "  \"\"\"Given a structured summary with clear recommendations, propose a business decision.\"\"\"\n",
    "\n",
    "@generative\n",
    "def generate_risk_mitigation(summary: str) -> str:\n",
    "  \"\"\"If the summary contains risk elements, propose mitigation strategies.\"\"\"\n",
    "\n",
    "@generative\n",
    "def generate_novel_recommendations(summary: str) -> str:\n",
    "  \"\"\"Provide a list of novel recommendations that are similar in plot or theme to the short story summary.\"\"\"\n",
    "\n",
    "# Compose the libraries.\n",
    "@generative\n",
    "def has_structured_conclusion(summary: str) -> Literal[\"yes\", \"no\"]:\n",
    "  \"\"\"Determine whether the summary contains a clearly marked conclusion or recommendation.\"\"\"\n",
    "\n",
    "@generative\n",
    "def contains_actionable_risks(summary: str) -> Literal[\"yes\", \"no\"]:\n",
    "  \"\"\"Check whether the summary contains references to business risks or exposure.\"\"\"\n",
    "\n",
    "@generative\n",
    "def has_theme_and_plot(summary: str) -> Literal[\"yes\", \"no\"]:\n",
    "  \"\"\"Check whether the summary contains both a plot and thematic elements.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89e1720d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 16:16:11-INFO ======\n",
      "Starting Mellea session: backend=ollama, model=granite4:micro, context=SimpleContext\u001b[0m\n",
      "Mitigation: Based on the summary provided, here are potential mitigation strategies for each market risk discussed in the Self-Sealing Stembolts Division: \n",
      "\n",
      "1. **25% Tariff on Micro-Carburetors**: Explore alternative suppliers that may not be subject to the same tariff rates or consider sourcing from regions with lower tariffs. Additionally, evaluate if there's a cost-effective way to pass these increased costs onto end consumers or explore bulk purchasing agreements with suppliers.\n",
      "\n",
      "2. **Increased Adoption of Self-Interlocking Leaf Screws**: Differentiate your product by offering higher quality and superior features that can outshine competitors in this category. Consider investing in research and development for innovative products, marketing efforts to highlight the benefits over self-interlocking leaf screws, or even forming strategic partnerships with tool manufacturers who produce compatible parts.\n",
      "\n",
      "3. **Growing Demand for Manual-Sealed Bolts**: Revive production of vintage tools that cater to the 'hipsterfication' trend in construction. This could involve conducting market research to understand consumer preferences better and tailoring your offerings accordingly. Consider also offering customization options or premium finishes to differentiate these products further.\n",
      "\n",
      "4. **Erosion in Use of Self-Sealing Stembolts**: Forecast potential demand for alternative fastening solutions as usage decreases. Monitor industry trends, engage with customers directly, and use this data to inform decisions about future product development.\n",
      "Decision: Addressing Market Risks for Self-Sealing Stembolts Division: Explore alternative suppliers for micro-carburetors, forecast erosion in use of self-sealing stembolts, revive production of vintage tools, and consider differentiating products through prestige lines.\n"
     ]
    }
   ],
   "source": [
    "m = start_session()\n",
    "transcript = \"\"\"Meeting Transcript: Market Risk Review -- Self-Sealing Stembolts Division\n",
    "Date: December 1, 3125\n",
    "Attendees:\n",
    "\n",
    "Karen Rojas, VP of Product Strategy\n",
    "\n",
    "Derek Madsen, Director of Global Procurement\n",
    "\n",
    "Felicia Zheng, Head of Market Research\n",
    "\n",
    "Tom Vega, CFO\n",
    "\n",
    "Luis Tran, Engineering Liaison\n",
    "\n",
    "Karen Rojas:\n",
    "Thanks, everyone, for making time on short notice. As you've all seen, we've got three converging market risks we need to address: tariffs on micro-carburetors, increased adoption of the self-interlocking leafscrew, and, believe it or not, the \"hipsterfication\" of the construction industry. I need all on deck and let's not waste time. Derek, start.\n",
    "\n",
    "Derek Madsen:\n",
    "Right. As of Monday, the 25% tariff on micro-carburetors sourced from the Pan-Alpha Centauri confederacy is active. We tried to pre-purchase a three-month buffer, but after that, our unit cost rises by $1.72. That's a 9% increase in the BOM cost of our core model 440 stembolt. Unless we find alternative suppliers or pass on the cost, we're eating into our already narrow margin.\n",
    "\n",
    "Tom Vega:\n",
    "We cannot absorb that without consequences. If we pass the cost downstream, we risk losing key mid-tier OEM clients. And with the market already sniffing around leafscrew alternatives, this makes us more vulnerable.\n",
    "\n",
    "Karen:\n",
    "Lets pause there. Felicia, give us the quick-and-dirty on the leafscrew.\n",
    "\n",
    "Felicia Zheng:\n",
    "It's ugly. Sales of the self-interlocking leafscrew—particularly in modular and prefab construction—are up 38% year-over-year. It's not quite a full substitute for our self-sealing stembolts, but they are close enough in function that some contractors are making the switch. Their appeal? No micro-carburetors, lower unit complexity, and easier training for install crews. We estimate we've lost about 12% of our industrial segment to the switch in the last two quarters.\n",
    "\n",
    "Karen:\n",
    "Engineering, Luis; your take on how real that risk is?\n",
    "\n",
    "Luis Tran:\n",
    "Technically, leafscrews are not as robust under high-vibration loads. But here's the thing: most of the modular prefab sites don not need that level of tolerance. If the design spec calls for durability over 10 years, we win. But for projects looking to move fast and hit 5-year lifespans? The leafscrew wins on simplicity and cost.\n",
    "\n",
    "Tom:\n",
    "So they're eating into our low-end. That's our volume base.\n",
    "\n",
    "Karen:\n",
    "Exactly. Now let's talk about this last one: the “hipsterfication” of construction. Felicia?\n",
    "\n",
    "Felicia:\n",
    "So this is wild. We're seeing a cultural shift in boutique and residential construction—especially in markets like Beckley, West Sullivan, parts of Osborne County, where clients are requesting \"authentic\" manual fasteners. They want hand-sealed bolts, visible threads, even mismatched patinas. It's an aesthetic thing. Function is almost secondary. Our old manual-seal line from the 3180s? People are hunting them down on auction sites.\n",
    "\n",
    "Tom:\n",
    "Well, I'm glad I don't have to live in the big cities... nothing like this would ever happen in downt-to-earth places Brooklyn, Portland, or Austin.\n",
    "\n",
    "Luis:\n",
    "We literally got a request from a design-build firm in Keough asking if we had any bolts “pre-distressed.”\n",
    "\n",
    "Karen:\n",
    "Can we spin this?\n",
    "\n",
    "Tom:\n",
    "If we keep our vintage tooling and market it right, maybe. But that's niche. It won't offset losses in industrial and prefab.\n",
    "\n",
    "Karen:\n",
    "Not yet. But we may need to reframe it as a prestige line—low volume, high margin. Okay, action items. Derek, map alternative micro-carburetor sources. Felicia, get me a forecast on leafscrew erosion by sector. Luis, feasibility of reviving manual seal production. Tom, let's scenario-plan cost pass-through vs. feature-based differentiation.\n",
    "\n",
    "Let's reconvene next week with hard numbers. Thanks, all.\"\"\"\n",
    "summary = summarize_meeting(m, transcript=transcript)\n",
    "\n",
    "if contains_actionable_risks(m, summary=summary) == \"yes\":\n",
    "    mitigation = generate_risk_mitigation(m, summary=summary)\n",
    "    print(f\"Mitigation: {mitigation}\")\n",
    "else:\n",
    "    print(\"Summary does not contain actionable risks.\")\n",
    "if has_structured_conclusion(m, summary=summary) == \"yes\":\n",
    "    decision = propose_business_decision(m, summary=summary)\n",
    "    print(f\"Decision: {decision}\")\n",
    "else:\n",
    "    print(\"Summary lacks a structured conclusion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f9b15",
   "metadata": {},
   "source": [
    "## **Mify Objects**\n",
    "\n",
    "```MObject``` storing data along side with its relevant operations(tools).This allows LLMs to interact with both tthe data and methods in a unifiied structured manner , simplifies exposinhg only specific fields and methods for LLMs to interact with.\n",
    "\n",
    "```Mobject``` pattern also provides a way of evolvoing exsisting classical codebases into generative programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88202f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 16:16:41-INFO ======\n",
      "Starting Mellea session: backend=ollama, model=granite4:micro, context=SimpleContext\u001b[0m\n",
      "The total sales for the East region, which includes both the Northeast and Southeast stores, can be calculated by adding their individual sales.\n",
      "\n",
      "Northeast store's sales = $250\n",
      "Southeast store's sales = $80\n",
      "\n",
      "Total sales in the East = Sales in Northeast + Sales in Southeast = $250 + $80 = $330\n"
     ]
    }
   ],
   "source": [
    "import mellea\n",
    "# old was mellea.stdlib.mify \n",
    "from mellea.stdlib.components.mify import mify, MifiedProtocol\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "@mify(fields_include = {\"table\"} , template = \"{{table}}\")\n",
    "class myDB:\n",
    "    table : str = \"\"\"| Store      | Sales   |\n",
    "                    | ---------- | ------- |\n",
    "                    | Northeast  | $250    |\n",
    "                    | Southeast  | $80     |\n",
    "                    | Midwest    | $420    |\"\"\"\n",
    "\n",
    "    def transpose(self):\n",
    "        pd.read_csv(\n",
    "            StringIO(self.table) , \n",
    "            sep = \"|\" ,\n",
    "            skipinitialspace= True ,\n",
    "            header = 0 ,\n",
    "            index_col = False \n",
    "        )\n",
    "\n",
    "m = mellea.start_session()\n",
    "db = myDB()\n",
    "assert isinstance(db , MifiedProtocol)\n",
    "answer = m.query( db , \" What is the total sales of the east ?\")\n",
    "print(str(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69c798",
   "metadata": {},
   "source": [
    "### How to find new updates in the code base that cause you to use older imports and throws errors ?\n",
    "```\n",
    "(mellea) ashoksharma@Ashoks-MacBook-Pro Mellea_IBM % cd mellea\n",
    "(mellea) ashoksharma@Ashoks-MacBook-Pro mellea % find . -name \"mify.py\"\n",
    "./mellea/stdlib/components/mify.py\n",
    "./docs/examples/mify/mify.py \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a1a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with documents\n",
    "\n",
    "from mellea.stdlib.components.docs.richdocument import RichDocument\n",
    "# Note: This requires the docling package installed\n",
    "\n",
    "rd = RichDocument.from_document_file(\"https://arxiv.org/pdf/1906.04043\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14ce8256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1: Cross-validated results of fake-text discriminators. Distributional information yield a higher informativeness than word-features in a logistic regression.\n",
      "\n",
      "| Feature                              | AUC         |\n",
      "|--------------------------------------|-------------|\n",
      "| Bag of Words                         | 0.63 ± 0.11 |\n",
      "| (Test 1 - GPT-2) Average Probability | 0.71 ± 0.25 |\n",
      "| (Test 2 - GPT-2) Top-K Buckets       | 0.87 ± 0.07 |\n",
      "| (Test 1 - BERT) Average Probability  | 0.70 ± 0.27 |\n",
      "| (Test 2 - BERT) Top-K Buckets        | 0.85 ± 0.09 |\n"
     ]
    }
   ],
   "source": [
    "# Example of working with a table from a document\n",
    "from mellea.stdlib.components.docs.richdocument import Table\n",
    "table1 : Table = rd.get_tables()[0] \n",
    "print(table1.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c28f2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Mellea Session...\n",
      "Session started! Model: granite4:micro\n",
      "\u001b[38;20m=== 16:18:32-INFO ======\n",
      "Tools for call: dict_keys(['from_markdown', 'to_markdown', 'transpose'])\u001b[0m\n",
      "====== TRYING AGAIN AS OUTPUT WAS NOT USEFUL\n",
      "\u001b[38;20m=== 16:18:55-INFO ======\n",
      "Tools for call: dict_keys(['from_markdown', 'to_markdown', 'transpose'])\u001b[0m\n",
      "\u001b[38;20m=== 16:19:10-INFO ======\n",
      "added a tool message from transform to the context\u001b[0m\n",
      "====== TRYING AGAIN AS OUTPUT WAS NOT USEFUL\n",
      "\u001b[38;20m=== 16:19:10-INFO ======\n",
      "Tools for call: dict_keys(['from_markdown', 'to_markdown', 'transpose'])\u001b[0m\n",
      "\u001b[38;20m=== 16:19:21-INFO ======\n",
      "added a tool message from transform to the context\u001b[0m\n",
      "| Feature                              | AUC         | Model   |\n",
      "|--------------------------------------|-------------|---------|\n",
      "| Bag of Words                         | 0.63 ± 0.11 | None    |\n",
      "| (Test 1 - GPT-2) Average Probability | 0.71 ± 0.25 | GPT-2   |\n",
      "| (Test 2 - GPT-2) Top-K Buckets       | 0.87 ± 0.07 | GPT-2   |\n",
      "| (Test 1 - BERT) Average Probability  | 0.70 ± 0.27 | BERT    |\n",
      "| (Test 2 - BERT) Top-K Buckets        | 0.85 ± 0.09 | BERT    |\n"
     ]
    }
   ],
   "source": [
    "import mellea\n",
    "from mellea.backends import ModelOption\n",
    "from mellea.backends.ollama import OllamaModelBackend\n",
    "from mellea import start_session\n",
    "\n",
    "print(\"Initializing Mellea Session...\")\n",
    "m = mellea.MelleaSession(\n",
    "    backend=OllamaModelBackend(\n",
    "        model_id=\"granite4:micro\", \n",
    "        model_options={ ModelOption.SEED : 42, ModelOption.MAX_NEW_TOKENS : 500} # Fixed seed for reproducibility\n",
    "    )\n",
    ")\n",
    "print(\"Session started! Model: granite4:micro\")\n",
    "\n",
    "op = (\"Add a column 'Model' that extracts which model was used or 'None' if none\")\n",
    "\n",
    "for seed in [x*12 for x in range(5)]:\n",
    "    table2 = m.transform(table1,op, model_options={ ModelOption.SEED : seed})\n",
    "    if isinstance(table2 , Table):\n",
    "        print(table2.to_markdown())\n",
    "        break\n",
    "    else :\n",
    "        print(f\"====== TRYING AGAIN AS OUTPUT WAS NOT USEFUL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af0de9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##MObjects methods as tools\n",
    "# when an object is mified all methods with a docstring get registered as tools for LLM calls\n",
    "# here only the from makrdown method will be exposed as tool to the llm (using funcs_include and funcs_exlude)\n",
    "\n",
    "from mellea.stdlib.components.mify import mify\n",
    "\n",
    "@mify(funcs_include = {\"from_markdown\"})\n",
    "class myDocLoader:\n",
    "    def __init__(self) -> None :\n",
    "        self.content = \" \"\n",
    "    \n",
    "    @classmethod\n",
    "    def from_markdown(cls , text : str) -> \"MyDocLoader\" :\n",
    "        doc = MyDocLoader()\n",
    "        #Your parsing func goes below\n",
    "        doc.content = text\n",
    "        return doc\n",
    "    def dummy(self) -> str:\n",
    "        return \" I am excluded \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "238078e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 16:19:22-INFO ======\n",
      "Tools for call: dict_keys(['from_markdown', 'to_markdown', 'transpose'])\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage of TableItem.export_to_dataframe() without `doc` argument is deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;20m=== 16:19:30-WARNING ======\n",
      "the transform of <mellea.stdlib.components.docs.richdocument.Table object at 0x3734e9370> with transformation description 'Transpose the table.' resulted in a tool call with no generated arguments; consider calling the function `transpose` directly\u001b[0m\n",
      "\u001b[38;20m=== 16:19:30-INFO ======\n",
      "added a tool message from transform to the context\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage of TableItem.export_to_dataframe() without `doc` argument is deprecated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|         | 0            | 1                                    | 2                              | 3                                   | 4                             |\n",
      "|---------|--------------|--------------------------------------|--------------------------------|-------------------------------------|-------------------------------|\n",
      "| Feature | Bag of Words | (Test 1 - GPT-2) Average Probability | (Test 2 - GPT-2) Top-K Buckets | (Test 1 - BERT) Average Probability | (Test 2 - BERT) Top-K Buckets |\n",
      "| AUC     | 0.63 ± 0.11  | 0.71 ± 0.25                          | 0.87 ± 0.07                    | 0.70 ± 0.27                         | 0.85 ± 0.09                   |\n",
      "|         | 0            | 1                                    | 2                              | 3                                   | 4                             |\n",
      "|---------|--------------|--------------------------------------|--------------------------------|-------------------------------------|-------------------------------|\n",
      "| Feature | Bag of Words | (Test 1 - GPT-2) Average Probability | (Test 2 - GPT-2) Top-K Buckets | (Test 1 - BERT) Average Probability | (Test 2 - BERT) Top-K Buckets |\n",
      "| AUC     | 0.63 ± 0.11  | 0.71 ± 0.25                          | 0.87 ± 0.07                    | 0.70 ± 0.27                         | 0.85 ± 0.09                   |\n"
     ]
    }
   ],
   "source": [
    "table1_v1 = m.transform(table1 , \"Transpose the table.\")\n",
    "table1_v2 = table1.transpose()\n",
    "print(table1_v1.to_markdown())\n",
    "print(table1_v2.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264aa895",
   "metadata": {},
   "source": [
    "### Debugging: Session Reset & Last Turn\n",
    "\n",
    "These are the 3 things you should check *all the time* when learning:\n",
    "\n",
    "1. `m.last_prompt()` → what did you actually send?\n",
    "2. `m.ctx.last_turn()` → last user+assistant pair\n",
    "3. `m.ctx.view_for_generation()` (advanced) → what context will be sent next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "715dba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Mellea Session...\n",
      "Session started! Model: granite4:micro\n"
     ]
    }
   ],
   "source": [
    "from mellea.backends import ModelOption\n",
    "from mellea import start_session\n",
    "\n",
    "print(\"Initializing Mellea Session...\")\n",
    "m = mellea.MelleaSession(\n",
    "    backend=OllamaModelBackend(\n",
    "        model_id=\"granite4:micro\", \n",
    "        model_options={ ModelOption.SEED: 42, ModelOption.MAX_NEW_TOKENS:500 } # Fixed seed for reproducibility\n",
    "    )\n",
    ")\n",
    "print(\"Session started! Model: granite4:micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0997e038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- last_prompt ---\n",
      "[{'role': 'user', 'content': 'For the following table please execute the following transformation. The table shown here is a representation of the actual `Table` object any tools will be run against.\\n\\nThe output you provide should be a markdown table or a function call that results in a `Table` object. \\n\\nIf using a tool, use the tool whose name most closely matches the transformation request:\\nTable:\\nTable 1: Cross-validated results of fake-text discriminators. Distributional information yield a higher informativeness than word-features in a logistic regression.\\n\\n| Feature                              | AUC         |\\n|--------------------------------------|-------------|\\n| Bag of Words                         | 0.63 ± 0.11 |\\n| (Test 1 - GPT-2) Average Probability | 0.71 ± 0.25 |\\n| (Test 2 - GPT-2) Top-K Buckets       | 0.87 ± 0.07 |\\n| (Test 1 - BERT) Average Probability  | 0.70 ± 0.27 |\\n| (Test 2 - BERT) Top-K Buckets        | 0.85 ± 0.09 |\\n\\nTransformation:\\nTranspose the table.', 'images': None}]\n",
      "\n",
      "--- last_turn ---\n",
      "ContextTurn(model_input=<mellea.stdlib.components.docs.richdocument.TableTransform object at 0x3717d8740>, output=ModelOutputThunk())\n"
     ]
    }
   ],
   "source": [
    "# Debugging commands\n",
    "print(\"--- last_prompt ---\")\n",
    "print(m.last_prompt())\n",
    "\n",
    "print(\"\\n--- last_turn ---\")\n",
    "print(m.ctx.last_turn() if hasattr(m, \"ctx\") else \"no ctx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00101c6c",
   "metadata": {},
   "source": [
    "## Context Management\n",
    "\n",
    "Mellea manages context using two complementary mechanisms:\n",
    "- Components themselves, which generally contain all of the context needed for a single-turn request. MObjects manage context using fields and methods, and Instructions have a grounding_context for RAG-style requests.\n",
    "- The Context, which stores and represents a (sometimes partial) history of all previous requests to the LLM made during the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d10c439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Mellea Session...\n",
      "Session started! Model: granite4:micro\n"
     ]
    }
   ],
   "source": [
    "import mellea\n",
    "from mellea.backends import ModelOption\n",
    "from mellea.backends.ollama import OllamaModelBackend\n",
    "from mellea.stdlib.context import SimpleContext, ChatContext \n",
    "#other here is chat Context\n",
    "from mellea import start_session\n",
    "\n",
    "\n",
    "# whenever we use start session we actually instiantiate Mellea with a default inference engine ,default model choice and a default Context Manager which is a simple Context Manager \n",
    "print(\"Initializing Mellea Session...\")\n",
    "m = mellea.MelleaSession(\n",
    "    backend=OllamaModelBackend(\n",
    "        model_id=\"granite4:micro\", \n",
    "    ),\n",
    "    ctx = SimpleContext()\n",
    ")\n",
    "print(\"Session started! Model: granite4:micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4cd2cc",
   "metadata": {},
   "source": [
    "The ```SimpleContext``` — which is the only context we have used so far — is a context manager that resets the chat message history on each model call. That is, the model’s context is entirely determined by the current Component. Mellea also provides a ```ChatContext```, which behaves like a chat history. We can use the ChatContext to interact with chat models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b8da533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 16:20:58-INFO ======\n",
      "Starting Mellea session: backend=ollama, model=granite4:micro, context=ChatContext\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mellea.Message(role=\"assistant\", content=\"Sure, let's solve it step by step.\n",
       "\n",
       "From equations (1) and (2), we can set them equal to each other:\n",
       "\n",
       "1/2 * Y = 1/3 * Z\n",
       "\n",
       "Multiplying both sides of the equation by 6 gives us:\n",
       "\n",
       "3Y = 2Z \n",
       "\n",
       "Now express Z in terms of Y:\n",
       "\n",
       "Z = (3/2)Y\n",
       "\n",
       "Next, substitute this expression for Z back into our sum equation:\n",
       "\n",
       "X + Y + (3/2)Y = 45\n",
       "\n",
       "Combine like terms:\n",
       "\n",
       "(1 + 1 + 3/2)Y = 45\n",
       "(5/2)Y = 45\n",
       "\n",
       "Now solve for Y by multiplying both sides of the equation by 2/5:\n",
       "\n",
       "Y = 90/5\n",
       "Y = 18\n",
       "\n",
       "Substitute Y back into our expression for Z:\n",
       "\n",
       "Z = (3/2)*18\n",
       "Z = 27\n",
       "\n",
       "Finally, substitute Y back into either equation (1) or (2) to find X. Let's use equation (1):\n",
       "\n",
       "X = 1/2 * Y\n",
       "X = 1/2 * 18\n",
       "X = 9\n",
       "\n",
       "So the three parts are:\n",
       "- First part (X): 9\n",
       "- Second part (Y): 18 \n",
       "- Third part (Z): 27\", images=\"[]\", documents=\"[]\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mellea import start_session\n",
    "\n",
    "m = mellea.start_session(ctx=ChatContext())\n",
    "m.chat(\"Make up a math problem.\")\n",
    "m.chat(\"Solve your math problem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8bba5",
   "metadata": {},
   "source": [
    "## Building Agents\n",
    "\n",
    "Mellea supports building agents like ReACT/Reasoning Loop agents. \n",
    "See `demo_agent.py` for a full implementation of a ReACT agent.\n",
    "\n",
    "## Interoperability\n",
    "\n",
    "- **MCP (Model Context Protocol)**: Expose Mellea tools to other AI clients. (See `demo_mcp.py`)\n",
    "- **crewAI Integration**: Mellea can be used as a backend for crewAI agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c63234",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "Most backends operate on text. For these backends/models, Mellea has an opinionated stance on how to transform Python objects into text: the TemplateFormatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba68c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Mellea Session...\n",
      "Session started! Model: granite4:micro\n"
     ]
    }
   ],
   "source": [
    "import docs.examples.generative_slots.generative_slots_with_requirements\n",
    "import asyncio.base_events\n",
    "import mellea\n",
    "from mellea.backends import ModelOption\n",
    "from mellea.backends.ollama import OllamaModelBackend\n",
    "\n",
    "# 1. Initialize Mellea with the local Ollama backend and Granite model\n",
    "# We use a fixed seed for reproducibility in this tutorial.\n",
    "print(\"Initializing Mellea Session...\")\n",
    "m = mellea.MelleaSession(\n",
    "    backend=OllamaModelBackend(\n",
    "        model_id=\"granite4:micro\", \n",
    "        model_options={ModelOption.SEED: 42} # Fixed seed for reproducibility\n",
    "    )\n",
    ")\n",
    "print(\"Session started! Model: granite4:micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edac8d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Flashcards...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 06:25:44-INFO ======\n",
      "SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:15<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw model output: {\n",
      "  \"topic\": \"OOP in Python\",\n",
      "  \"cards\": [\n",
      "    {\n",
      "      \"question\": \"What is the role of 'class' keyword in OOP?\",\n",
      "      \"answer\": \"'class' keyword is used to define a new class and introduce its members like attributes, methods.\",\n",
      "      \"difficulty\": \"easy\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How are objects created from classes defined by a user?\",\n",
      "      \"answer\": \"Objects are created using the syntax: ClassName(arguments)\",\n",
      "      \"difficulty\": \"medium\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What is encapsulation in OOP?\",\n",
      "      \"answer\": \"Encapsulation is the process of hiding the internal state of an object and exposing only what's necessary, often done by creating private properties.\",\n",
      "      \"difficulty\": \"hard\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Parsed Object: topic='OOP in Python' cards=[{'question': \"What is the role of 'class' keyword in OOP?\", 'answer': \"'class' keyword is used to define a new class and introduce its members like attributes, methods.\", 'difficulty': 'easy'}, {'question': 'How are objects created from classes defined by a user?', 'answer': 'Objects are created using the syntax: ClassName(arguments)', 'difficulty': 'medium'}, {'question': 'What is encapsulation in OOP?', 'answer': \"Encapsulation is the process of hiding the internal state of an object and exposing only what's necessary, often done by creating private properties.\", 'difficulty': 'hard'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mellea.core import Component, CBlock\n",
    "import pydantic\n",
    "\n",
    "# Define a structured output format using Pydantic\n",
    "class FlashcardSetOut(pydantic.BaseModel):\n",
    "    topic: str\n",
    "    cards: list[dict]\n",
    "\n",
    "# Define a custom Component\n",
    "class FlashcardMaker(Component):\n",
    "    def __init__(self, topic: str, count: int = 5):\n",
    "        self.topic = topic\n",
    "        self.count = count\n",
    "\n",
    "    def parts(self):\n",
    "        # We don't have nested parts in this simple example.\n",
    "        return []\n",
    "    \n",
    "    # Along with a template, each class/object needs to define the arguments that will be supplied when rendering the template.This happens here format_for_llm\n",
    "    # It returns either a string or a TemplateRepresentation(obj, args, template).\n",
    "\n",
    "    def format_for_llm(self):\n",
    "        # Returning a STRING is the simplest valid way to represent a Component.\n",
    "        return (\n",
    "            \"You are a study assistant.\\n\"\n",
    "            f\"Make {self.count} flashcards about: {self.topic}.\\n\"\n",
    "            \"Return JSON with keys: topic (string), cards (list of objects).\\n\"\n",
    "            \"Each card object must have: question, answer, difficulty.\\n\"\n",
    "        )\n",
    "\n",
    "    def _parse(self, computed):\n",
    "        # In a real component, you might parse the JSON here.\n",
    "        # For this tutorial, we return the raw value and parse it manually outside.\n",
    "        return computed.value\n",
    "\n",
    "# Act on the component\n",
    "# m.act(component) → model sees the component as the “action”\n",
    "\n",
    "# m.instruct(\"...\", grounding_context={...}) → model sees your instruction plus the rendered grounding context (using your templates).\n",
    "print(\"Generating Flashcards...\")\n",
    "res = m.act(FlashcardMaker(\"OOP in Python\", count=3), format=FlashcardSetOut)\n",
    "\n",
    "print(\"\\nRaw model output:\", res.value)\n",
    "\n",
    "# Parse it back into the Pydantic model\n",
    "parsed = FlashcardSetOut.model_validate_json(res.value)\n",
    "print(\"\\nParsed Object:\", parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12508605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mellea\n",
    "from mellea.core import TemplateRepresentation, ModelOutputThunk\n",
    "from mellea.stdlib.components.docs.richdocument import RichDocument\n",
    "import pydantic\n",
    "\n",
    "class AbstractOut(pydantic.BaseModel):\n",
    "    title_guess : str \n",
    "    one_paragraph_abtract : str\n",
    "\n",
    "#\n",
    "class PoorDocumentExtractor(RichDocument):\n",
    "    \n",
    "    \"\"\" takes only heading and gives an appropriate summary based on that \"\"\"\n",
    "   \n",
    "   # just formatting titles nicely\n",
    "    def format_for_llm(self) -> TemplateRepresentation :\n",
    "        titles = []\n",
    "        md = self.to_markdown()\n",
    "        for line in md.splitlines():\n",
    "            if line.startswith(\"#\"):\n",
    "                titles.append(line.lstrip(\"#\").strip())\n",
    "\n",
    "        template = (\n",
    "            \"You will see section titles from a document.\\n\"\n",
    "            \"Infer what the document is about.\\n\\n\"\n",
    "            \"{% for t in titles %}- {{ t }}\\n{% endfor %}\"\n",
    "        )\n",
    "\n",
    "        return TemplateRepresentation( obj = self , args = {\"titles\" : titles} , template = template)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56c789f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 06:37:05-INFO ======\n",
      "Starting Mellea session: backend=ollama, model=granite4:micro, context=SimpleContext\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m doc = PoorDocumentExtractor.from_document_file(source)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Composed prompt: small instruction + grounded doc view\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m out = \u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minstruct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBased only on [DOC_TITLES], guess the paper topic and write a one-paragraph abstract.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReturn JSON with: title_guess, one_paragraph_abstract.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrounding_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDOC_TITLES\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mAbstractOut\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# out might be ModelOutputThunk; print raw + parsed\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, ModelOutputThunk):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/stdlib/session.py:420\u001b[39m, in \u001b[36mMelleaSession.instruct\u001b[39m\u001b[34m(self, description, images, requirements, icl_examples, grounding_context, user_variables, prefix, output_prefix, strategy, return_sampling_results, format, model_options, tool_calls)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minstruct\u001b[39m(\n\u001b[32m    387\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    388\u001b[39m     description: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    401\u001b[39m     tool_calls: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    402\u001b[39m ) -> ModelOutputThunk[\u001b[38;5;28mstr\u001b[39m] | SamplingResult:\n\u001b[32m    403\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generates from an instruction.\u001b[39;00m\n\u001b[32m    404\u001b[39m \n\u001b[32m    405\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    418\u001b[39m \u001b[33;03m        images: A list of images to be used in the instruction or None if none.\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     r = \u001b[43mmfuncs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minstruct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43micl_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[43micl_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrounding_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrounding_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_sampling_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_sampling_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, SamplingResult):\n\u001b[32m    439\u001b[39m         \u001b[38;5;28mself\u001b[39m.ctx = r.result_ctx\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/stdlib/functional.py:211\u001b[39m, in \u001b[36minstruct\u001b[39m\u001b[34m(description, context, backend, images, requirements, icl_examples, grounding_context, user_variables, prefix, output_prefix, strategy, return_sampling_results, format, model_options, tool_calls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# All instruction options are forwarded to create a new Instruction object.\u001b[39;00m\n\u001b[32m    200\u001b[39m i = Instruction(\n\u001b[32m    201\u001b[39m     description=description,\n\u001b[32m    202\u001b[39m     requirements=requirements,\n\u001b[32m   (...)\u001b[39m\u001b[32m    208\u001b[39m     images=images,\n\u001b[32m    209\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_sampling_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_sampling_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/stdlib/functional.py:92\u001b[39m, in \u001b[36mact\u001b[39m\u001b[34m(action, context, backend, requirements, strategy, return_sampling_results, format, model_options, tool_calls)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mact\u001b[39m(\n\u001b[32m     65\u001b[39m     action: Component[S],\n\u001b[32m     66\u001b[39m     context: Context,\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m     tool_calls: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     75\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ModelOutputThunk[S], Context] | SamplingResult[S]:\n\u001b[32m     76\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs a generic action, and adds both the action and the result to the context.\u001b[39;00m\n\u001b[32m     77\u001b[39m \n\u001b[32m     78\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[33;03m        A (ModelOutputThunk, Context) if `return_sampling_results` is `False`, else returns a `SamplingResult`.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     out = \u001b[43m_run_async_in_thread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43maact\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m            \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_sampling_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_sampling_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m            \u001b[49m\u001b[43msilence_context_type_warning\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We can safely silence this here since it's in a sync function.\u001b[39;49;00m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Mypy doesn't like the bool for return_sampling_results.\u001b[39;49;00m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/helpers/event_loop_helper.py:89\u001b[39m, in \u001b[36m_run_async_in_thread\u001b[39m\u001b[34m(co)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_async_in_thread\u001b[39m(co: Coroutine[Any, Any, R]) -> R:\n\u001b[32m     69\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call to run async code from synchronous code in Mellea.\u001b[39;00m\n\u001b[32m     70\u001b[39m \n\u001b[32m     71\u001b[39m \u001b[33;03m    In Mellea, we utilize async code underneath sync code to speed up\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m \u001b[33;03m        output of the coroutine\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__event_loop_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mco\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/helpers/event_loop_helper.py:61\u001b[39m, in \u001b[36m_EventLoopHandler.__call__\u001b[39m\u001b[34m(self, co)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_loop == get_current_event_loop():\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# If this gets called from the same event loop, launch in a separate thread to prevent blocking.\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _EventLoopHandler()(co)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_coroutine_threadsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mco\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_event_loop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/stdlib/functional.py:517\u001b[39m, in \u001b[36maact\u001b[39m\u001b[34m(action, context, backend, requirements, strategy, return_sampling_results, format, model_options, tool_calls, silence_context_type_warning)\u001b[39m\n\u001b[32m    511\u001b[39m     generate_logs.append(result._generate_log)\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    514\u001b[39m     \u001b[38;5;66;03m# Always sample if a strategy is provided, even if no requirements were provided.\u001b[39;00m\n\u001b[32m    515\u001b[39m     \u001b[38;5;66;03m# Some sampling strategies don't use requirements or set them when instantiated.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     sampling_result = \u001b[38;5;28;01mawait\u001b[39;00m strategy.sample(\n\u001b[32m    518\u001b[39m         action,\n\u001b[32m    519\u001b[39m         context=context,\n\u001b[32m    520\u001b[39m         backend=backend,\n\u001b[32m    521\u001b[39m         requirements=requirements,\n\u001b[32m    522\u001b[39m         validation_ctx=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    523\u001b[39m         \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28mformat\u001b[39m,\n\u001b[32m    524\u001b[39m         model_options=model_options,\n\u001b[32m    525\u001b[39m         tool_calls=tool_calls,\n\u001b[32m    526\u001b[39m     )\n\u001b[32m    528\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sampling_result.sample_generations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m sampling_result.sample_generations:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/stdlib/sampling/base.py:163\u001b[39m, in \u001b[36mBaseSamplingStrategy.sample\u001b[39m\u001b[34m(self, action, context, backend, requirements, validation_ctx, format, model_options, tool_calls, show_progress)\u001b[39m\n\u001b[32m    160\u001b[39m     flog.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning loop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloop_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.loop_budget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# run a generation pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m result, result_ctx = \u001b[38;5;28;01mawait\u001b[39;00m backend.generate_from_context(\n\u001b[32m    164\u001b[39m     next_action,\n\u001b[32m    165\u001b[39m     ctx=next_context,\n\u001b[32m    166\u001b[39m     \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28mformat\u001b[39m,\n\u001b[32m    167\u001b[39m     model_options=model_options,\n\u001b[32m    168\u001b[39m     tool_calls=tool_calls,\n\u001b[32m    169\u001b[39m )\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m result.avalue()\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m# Sampling strategies may use different components from the original\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# action. This might cause discrepancies in the expected parsed_repr\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# type / value. Explicitly overwrite that here.\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# TODO: See if there's a more elegant way for this so that each sampling\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# strategy doesn't have to re-implement it.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/backends/ollama.py:261\u001b[39m, in \u001b[36mOllamaModelBackend.generate_from_context\u001b[39m\u001b[34m(self, action, ctx, format, model_options, tool_calls)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"See `generate_from_chat_context`.\"\"\"\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m ctx.is_chat_context, (\n\u001b[32m    259\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe ollama backend only supports chat-like contexts.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    260\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m mot = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate_from_chat_context(\n\u001b[32m    262\u001b[39m     action,\n\u001b[32m    263\u001b[39m     ctx,\n\u001b[32m    264\u001b[39m     _format=\u001b[38;5;28mformat\u001b[39m,\n\u001b[32m    265\u001b[39m     model_options=model_options,\n\u001b[32m    266\u001b[39m     tool_calls=tool_calls,\n\u001b[32m    267\u001b[39m )\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mot, ctx.add(action).add(mot)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/backends/ollama.py:308\u001b[39m, in \u001b[36mOllamaModelBackend.generate_from_chat_context\u001b[39m\u001b[34m(self, action, ctx, _format, model_options, tool_calls)\u001b[39m\n\u001b[32m    304\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m    305\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe ollama backend does not support currently support activated LoRAs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    306\u001b[39m         )\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m         messages.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_chat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# construct the conversation from our messages, adding a system prompt at the first message if one was provided.\u001b[39;00m\n\u001b[32m    310\u001b[39m conversation: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/formatters/chat_formatter.py:56\u001b[39m, in \u001b[36mChatFormatter.to_chat_messages\u001b[39m\u001b[34m(self, cs)\u001b[39m\n\u001b[32m     53\u001b[39m         \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[32m     54\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m Message(role=role, content=\u001b[38;5;28mself\u001b[39m.print(c))\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_to_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/formatters/chat_formatter.py:52\u001b[39m, in \u001b[36mChatFormatter.to_chat_messages.<locals>._to_msg\u001b[39m\u001b[34m(c)\u001b[39m\n\u001b[32m     49\u001b[39m         images = tr.images\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# components can have images\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Message(role=role, content=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m, images=images)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Message(role=role, content=\u001b[38;5;28mself\u001b[39m.print(c))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/formatters/template_formatter.py:114\u001b[39m, in \u001b[36mTemplateFormatter.print\u001b[39m\u001b[34m(self, c)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m, c: Component | CBlock) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    113\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses a jinja2 template to pretty-print components.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stringify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/formatters/template_formatter.py:81\u001b[39m, in \u001b[36mTemplateFormatter._stringify\u001b[39m\u001b[34m(self, c)\u001b[39m\n\u001b[32m     79\u001b[39m stringified_template_args = {}\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m representation.args.items():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     stringified_template_args[key] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stringify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m representation.obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     84\u001b[39m     FancyLogger.get_logger().warning(\n\u001b[32m     85\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtemplate formatter encountered a TemplateRepresentation with no obj when stringifying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m; setting obj to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/formatters/template_formatter.py:95\u001b[39m, in \u001b[36mTemplateFormatter._stringify\u001b[39m\u001b[34m(self, c)\u001b[39m\n\u001b[32m     93\u001b[39m     stringified_template_args = {}\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m c.items():\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         stringified_template_args[key] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stringify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stringified_template_args\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m c \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, Iterable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, Mapping):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Folder/pythonfiles/Mellea_IBM/mellea/mellea/formatters/template_formatter.py:74\u001b[39m, in \u001b[36mTemplateFormatter._stringify\u001b[39m\u001b[34m(self, c)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m c.value\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m Component():\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     representation = \u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_for_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(representation) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m representation\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mPoorDocumentExtractor.format_for_llm\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     19\u001b[39m         titles.append(line.lstrip(\u001b[33m\"\u001b[39m\u001b[33m#\u001b[39m\u001b[33m\"\u001b[39m).strip())\n\u001b[32m     21\u001b[39m template = (\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mYou will see section titles from a document.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mInfer what the document is about.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[38;5;132;01m% f\u001b[39;00m\u001b[33mor t in titles \u001b[39m\u001b[33m%\u001b[39m\u001b[33m}- \u001b[39m\u001b[33m{{\u001b[39m\u001b[33m t }}\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m{\u001b[39m\u001b[38;5;132;01m% e\u001b[39;00m\u001b[33mndfor \u001b[39m\u001b[33m%\u001b[39m\u001b[33m}\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m TemplateRepresentation( obj = \u001b[38;5;28mself\u001b[39m , args = {titles} , template = template)\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "m = mellea.start_session()\n",
    "source = \"https://arxiv.org/pdf/1906.04043\"\n",
    "\n",
    "doc = PoorDocumentExtractor.from_document_file(source)\n",
    "\n",
    "# Composed prompt: small instruction + grounded doc view\n",
    "out = m.instruct(\n",
    "    \"Based only on [DOC_TITLES], guess the paper topic and write a one-paragraph abstract.\\n\"\n",
    "    \"Return JSON with: title_guess, one_paragraph_abstract.\",\n",
    "    grounding_context={\"DOC_TITLES\": doc},\n",
    "    format=AbstractOut,\n",
    ")\n",
    "\n",
    "# out might be ModelOutputThunk; print raw + parsed\n",
    "if isinstance(out, ModelOutputThunk):\n",
    "    print(\"RAW:\\n\", out.value)\n",
    "    parsed = AbstractOut.model_validate_json(out.value)\n",
    "    print(\"\\nPARSED:\\n\", parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea83fc22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
