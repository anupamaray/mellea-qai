{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Mellea Complete Tutorial\n",
                "\n",
                "Welcome to the comprehensive interactive tutorial for **Mellea**.\n",
                "This notebook combines practical usage patterns with core architectural concepts.\n",
                "\n",
                "**Docs Reference:** [Welcome](https://docs.mellea.ai/overview/mellea-welcome), [Quickstart](https://docs.mellea.ai/overview/quick-start)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import mellea\n",
                "from mellea.backends import ModelOption\n",
                "from mellea.backends.ollama import OllamaModelBackend\n",
                "\n",
                "# 1. Initialize Mellea with the local Ollama backend and Granite model\n",
                "# We use a fixed seed for reproducibility in this tutorial.\n",
                "print(\"Initializing Mellea Session...\")\n",
                "m = mellea.MelleaSession(\n",
                "    backend=OllamaModelBackend(\n",
                "        model_id=\"granite4:micro\", \n",
                "        model_options={ModelOption.SEED: 42} # Fixed seed for reproducibility\n",
                "    )\n",
                ")\n",
                "print(\"Session started! Model: granite4:micro\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Basic Instruction\n",
                "\n",
                "The core of Mellea is the `instruct` method. You give it a natural language instruction, and it returns a result."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "response = m.instruct(\"Write a short haiku about writing python code.\")\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Components & CBlocks (Core Concepts)\n",
                "\n",
                "Most LLM libraries just pass strings around. Mellea is different: it uses **Components**.\n",
                "\n",
                "A `Component` is a structured object Mellea can format for an LLM call.\n",
                "\n",
                "The minimum you need to define a custom component is:\n",
                "- `parts()` → list of sub-parts (other Components or CBlocks)\n",
                "- `format_for_llm()` → string (or TemplateRepresentation)\n",
                "\n",
                "Then you can run it with `m.act(component)`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mellea.core import Component, CBlock\n",
                "import pydantic\n",
                "\n",
                "# Define a structured output format using Pydantic\n",
                "class FlashcardSetOut(pydantic.BaseModel):\n",
                "    topic: str\n",
                "    cards: list[dict]\n",
                "\n",
                "# Define a custom Component\n",
                "class FlashcardMaker(Component):\n",
                "    def __init__(self, topic: str, count: int = 5):\n",
                "        self.topic = topic\n",
                "        self.count = count\n",
                "\n",
                "    def parts(self):\n",
                "        # We don't have nested parts in this simple example.\n",
                "        return []\n",
                "\n",
                "    def format_for_llm(self):\n",
                "        # Returning a STRING is the simplest valid way to represent a Component.\n",
                "        return (\n",
                "            \"You are a study assistant.\\n\"\n",
                "            f\"Make {self.count} flashcards about: {self.topic}.\\n\"\n",
                "            \"Return JSON with keys: topic (string), cards (list of objects).\\n\"\n",
                "            \"Each card object must have: question, answer, difficulty.\\n\"\n",
                "        )\n",
                "\n",
                "    def _parse(self, computed):\n",
                "        # In a real component, you might parse the JSON here.\n",
                "        # For this tutorial, we return the raw value and parse it manually outside.\n",
                "        return computed.value\n",
                "\n",
                "# Act on the component\n",
                "print(\"Generating Flashcards...\")\n",
                "res = m.act(FlashcardMaker(\"OOP in Python\", count=3), format=FlashcardSetOut)\n",
                "\n",
                "print(\"\\nRaw model output:\", res.value)\n",
                "\n",
                "# Parse it back into the Pydantic model\n",
                "parsed = FlashcardSetOut.model_validate_json(res.value)\n",
                "print(\"\\nParsed Object:\", parsed)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# The Loop: Instruct-Validate-Repair (Docs: Requirements)\n",
                "\n",
                "Generative models can be unpredictable. Mellea's power lies in its **Instruct-Validate-Repair** loop. \n",
                "You can define **Requirements** (`req`) and **Checks** (`check`) that the output *must* satisfy. If the model fails, Mellea uses a **Strategy** to try again or fix it.\n",
                "\n",
                "**Docs Reference:** [Requirements](https://docs.mellea.ai/overview/requirements)\n",
                "\n",
                "### Example: Generating a Helper Email with Constraints\n",
                "We want to generate an email that:\n",
                "1.  Has a salutation.\n",
                "2.  Does **not** use the word 'regards'.\n",
                "3.  (Optional strict check) Uses only lower-case letters (just to show validation failure and repair)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mellea.stdlib.sampling import RejectionSamplingStrategy\n",
                "from mellea.stdlib.requirements import req,check,simple_validate\n",
                "\n",
                "# using a validation fn to validate your results as well as a check to validate your results\n",
                "requirements = [\n",
                "    req(\"The email should have a salutation\"),\n",
                "    req(\"Use only lower-case letters\", validation_fn=simple_validate(lambda s: s.islower())),\n",
                "    check(\"The email should not mention the word 'regards'\")\n",
                "        \n",
                "]\n",
                "\n",
                "def write_email(m:mellea.MelleaSession, names :str , notes:str)->str:\n",
                "    email_candidate = m.instruct(\n",
                "        \"Write an email to {{name}} using the notes following: {{notes}}.\",\n",
                "        requirements=requirements,\n",
                "        strategy = RejectionSamplingStrategy(loop_budget = 5),\n",
                "        user_variables={\"name\": names, \"notes\": notes},\n",
                "        return_sampling_results = True ,\n",
                "    )\n",
                "    if email_candidate.success :\n",
                "        return str(email_candidate.result)\n",
                "    else :\n",
                "        print(\"Expect sub-par output due to failure to meet requirements.\")\n",
                "        return email_candidate.sampling_results[0].value\n",
                "    \n",
                "m=mellea.start_session()\n",
                "print(write_email(m,\"Arush\",\"Arush has been a great team player, always willing to help others and contribute to group projects.\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Advanced: Core Concepts & Generative Programming\n",
                "\n",
                "**Docs Reference:** [Generative Programming](https://docs.mellea.ai/overview/project-mellea), [Core Concepts](https://docs.mellea.ai/core-concept/generative-slots)\n",
                "\n",
                "In this section, we touch on more advanced concepts. Mellea treats prompts not just as strings, but as programs with **Slots**, **Context**, and **Agents**.\n",
                "\n",
                "### Variable Injection (Context)\n",
                "You saw `user_variables` above. This is part of Mellea's Context Management system, allowing you to inject data safely into prompts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick example of dynamic variables\n",
                "response = m.instruct(\n",
                "    \"Translate the following word to Spanish: {{word}}\",\n",
                "    user_variables={\"word\": \"Hello\"}\n",
                ")\n",
                "print(f\"Hello in Spanish: {response}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# using model options\n",
                "import mellea\n",
                "from mellea.backends import ModelOption  #its mellea.backends\n",
                "from mellea.backends.ollama import OllamaModelBackend\n",
                "from mellea.backends import model_ids\n",
                "\n",
                "m = mellea.MelleaSession(\n",
                "    backend=OllamaModelBackend(\n",
                "        model_id=\"granite4:micro\",model_options={ModelOption.SEED: 42}\n",
                "    )\n",
                ")\n",
                "\n",
                "answer = m.instruct(\n",
                "    \"What is 2x2?\",\n",
                "    model_options={\n",
                "        \"temperature\": 0.1,\n",
                "    },\n",
                ")\n",
                "\n",
                "print(str(answer))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generative Slots\n",
                "\n",
                "``GenerativeSlot`` is a function whose implementation is provieded by an LLM.In Mellea you define these using ``@generative`` decorator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from mellea.std.base import ChatContext is old \n",
                "from mellea.stdlib.context import ChatContext\n",
                "from mellea import start_session\n",
                "from mellea.backends import ModelOption\n",
                "from mellea import generative   \n",
                "\n",
                "# ChatContext is used to maintain conversation history across multiple model calls. \n",
                "# Unlike SimpleContext (the default), which resets the chat history on each call, \n",
                "# ChatContext behaves like a chat history where previous messages are remembered.\n",
                "\n",
                "@generative\n",
                "def grade_syntax(code: str) -> int:\n",
                "    \"\"\" Grade the code based on correct implementation of the function\n",
                "    args:\n",
                "        code: str (to be graded)\n",
                "    returns:\n",
                "        int : a grade between 1(worst) and 10(best)\n",
                "    \"\"\"\n",
                "codes = (\n",
                "    \" def add(a, b):\\n    return a + b\",\n",
                "    \"def subtract(a,b): cout<<a-b<<endl\",\n",
                "    \"int multiply(int a,int b) { return a*b}\"\n",
                ")\n",
                "m = start_session(ctx = ChatContext() , model_options = {ModelOption.MAX_NEW_TOKENS: 10})\n",
                "for code in codes :\n",
                "    grade = grade_syntax(m, code = code)\n",
                "    print(\"grade = \",grade)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Advanced: Using Generative slots to Provide Compositionality Across Module Boundaries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# file: https://github.com/generative-computing/mellea/blob/main/docs/examples/tutorial/compositionality_with_generative_slots.py#L1-L18\n",
                "from mellea import generative\n",
                "from typing import Literal\n",
                "\n",
                "# The Summarizer Library\n",
                "@generative\n",
                "def summarize_meeting(transcript: str) -> str:\n",
                "  \"\"\"Summarize the meeting transcript into a concise paragraph of main points.\"\"\"\n",
                "\n",
                "@generative\n",
                "def summarize_contract(contract_text: str) -> str:\n",
                "  \"\"\"Produce a natural language summary of contract obligations and risks.\"\"\"\n",
                "\n",
                "@generative\n",
                "def summarize_short_story(story: str) -> str:\n",
                "  \"\"\"Summarize a short story, with one paragraph on plot and one paragraph on broad themes.\"\"\"\n",
                "\n",
                "\n",
                "# The Decision Aides Library\n",
                "@generative\n",
                "def propose_business_decision(summary: str) -> str:\n",
                "  \"\"\"Given a structured summary with clear recommendations, propose a business decision.\"\"\"\n",
                "\n",
                "@generative\n",
                "def generate_risk_mitigation(summary: str) -> str:\n",
                "  \"\"\"If the summary contains risk elements, propose mitigation strategies.\"\"\"\n",
                "\n",
                "@generative\n",
                "def generate_novel_recommendations(summary: str) -> str:\n",
                "  \"\"\"Provide a list of novel recommendations that are similar in plot or theme to the short story summary.\"\"\"\n",
                "\n",
                "# Compose the libraries.\n",
                "@generative\n",
                "def has_structured_conclusion(summary: str) -> Literal[\"yes\", \"no\"]:\n",
                "  \"\"\"Determine whether the summary contains a clearly marked conclusion or recommendation.\"\"\"\n",
                "\n",
                "@generative\n",
                "def contains_actionable_risks(summary: str) -> Literal[\"yes\", \"no\"]:\n",
                "  \"\"\"Check whether the summary contains references to business risks or exposure.\"\"\"\n",
                "\n",
                "@generative\n",
                "def has_theme_and_plot(summary: str) -> Literal[\"yes\", \"no\"]:\n",
                "  \"\"\"Check whether the summary contains both a plot and thematic elements.\"\"\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "m = start_session()\n",
                "transcript = \"\"\"Meeting Transcript: Market Risk Review -- Self-Sealing Stembolts Division\n",
                "Date: December 1, 3125\n",
                "Attendees:\n",
                "\n",
                "Karen Rojas, VP of Product Strategy\n",
                "\n",
                "Derek Madsen, Director of Global Procurement\n",
                "\n",
                "Felicia Zheng, Head of Market Research\n",
                "\n",
                "Tom Vega, CFO\n",
                "\n",
                "Luis Tran, Engineering Liaison\n",
                "\n",
                "Karen Rojas:\n",
                "Thanks, everyone, for making time on short notice. As you've all seen, we've got three converging market risks we need to address: tariffs on micro-carburetors, increased adoption of the self-interlocking leafscrew, and, believe it or not, the \"hipsterfication\" of the construction industry. I need all on deck and let's not waste time. Derek, start.\n",
                "\n",
                "Derek Madsen:\n",
                "Right. As of Monday, the 25% tariff on micro-carburetors sourced from the Pan-Alpha Centauri confederacy is active. We tried to pre-purchase a three-month buffer, but after that, our unit cost rises by $1.72. That's a 9% increase in the BOM cost of our core model 440 stembolt. Unless we find alternative suppliers or pass on the cost, we're eating into our already narrow margin.\n",
                "\n",
                "Tom Vega:\n",
                "We cannot absorb that without consequences. If we pass the cost downstream, we risk losing key mid-tier OEM clients. And with the market already sniffing around leafscrew alternatives, this makes us more vulnerable.\n",
                "\n",
                "Karen:\n",
                "Lets pause there. Felicia, give us the quick-and-dirty on the leafscrew.\n",
                "\n",
                "Felicia Zheng:\n",
                "It's ugly. Sales of the self-interlocking leafscrew—particularly in modular and prefab construction—are up 38% year-over-year. It's not quite a full substitute for our self-sealing stembolts, but they are close enough in function that some contractors are making the switch. Their appeal? No micro-carburetors, lower unit complexity, and easier training for install crews. We estimate we've lost about 12% of our industrial segment to the switch in the last two quarters.\n",
                "\n",
                "Karen:\n",
                "Engineering, Luis; your take on how real that risk is?\n",
                "\n",
                "Luis Tran:\n",
                "Technically, leafscrews are not as robust under high-vibration loads. But here's the thing: most of the modular prefab sites don not need that level of tolerance. If the design spec calls for durability over 10 years, we win. But for projects looking to move fast and hit 5-year lifespans? The leafscrew wins on simplicity and cost.\n",
                "\n",
                "Tom:\n",
                "So they're eating into our low-end. That's our volume base.\n",
                "\n",
                "Karen:\n",
                "Exactly. Now let's talk about this last one: the “hipsterfication” of construction. Felicia?\n",
                "\n",
                "Felicia:\n",
                "So this is wild. We're seeing a cultural shift in boutique and residential construction—especially in markets like Beckley, West Sullivan, parts of Osborne County, where clients are requesting \"authentic\" manual fasteners. They want hand-sealed bolts, visible threads, even mismatched patinas. It's an aesthetic thing. Function is almost secondary. Our old manual-seal line from the 3180s? People are hunting them down on auction sites.\n",
                "\n",
                "Tom:\n",
                "Well, I'm glad I don't have to live in the big cities... nothing like this would ever happen in downt-to-earth places Brooklyn, Portland, or Austin.\n",
                "\n",
                "Luis:\n",
                "We literally got a request from a design-build firm in Keough asking if we had any bolts “pre-distressed.”\n",
                "\n",
                "Karen:\n",
                "Can we spin this?\n",
                "\n",
                "Tom:\n",
                "If we keep our vintage tooling and market it right, maybe. But that's niche. It won't offset losses in industrial and prefab.\n",
                "\n",
                "Karen:\n",
                "Not yet. But we may need to reframe it as a prestige line—low volume, high margin. Okay, action items. Derek, map alternative micro-carburetor sources. Felicia, get me a forecast on leafscrew erosion by sector. Luis, feasibility of reviving manual seal production. Tom, let's scenario-plan cost pass-through vs. feature-based differentiation.\n",
                "\n",
                "Let's reconvene next week with hard numbers. Thanks, all.\"\"\"\n",
                "summary = summarize_meeting(m, transcript=transcript)\n",
                "\n",
                "if contains_actionable_risks(m, summary=summary) == \"yes\":\n",
                "    mitigation = generate_risk_mitigation(m, summary=summary)\n",
                "    print(f\"Mitigation: {mitigation}\")\n",
                "else:\n",
                "    print(\"Summary does not contain actionable risks.\")\n",
                "if has_structured_conclusion(m, summary=summary) == \"yes\":\n",
                "    decision = propose_business_decision(m, summary=summary)\n",
                "    print(f\"Decision: {decision}\")\n",
                "else:\n",
                "    print(\"Summary lacks a structured conclusion.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Mify Objects\n",
                "\n",
                "```MObject``` storing data along side with its relevant operations(tools).This allows LLMs to interact with both tthe data and methods in a unifiied structured manner , simplifies exposinhg only specific fields and methods for LLMs to interact with.\n",
                "\n",
                "```Mobject``` pattern also provides a way of evolvoing exsisting classical codebases into generative programs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import mellea\n",
                "# old was mellea.stdlib.mify \n",
                "from mellea.stdlib.components.mify import mify, MifiedProtocol\n",
                "import pandas as pd\n",
                "from io import StringIO\n",
                "\n",
                "@mify(fields_include = {\"table\"} , template = \"{{table}}\")\n",
                "class myDB:\n",
                "    table : str = \"\"\"| Store      | Sales   |\n",
                "                    | ---------- | ------- |\n",
                "                    | Northeast  | $250    |\n",
                "                    | Southeast  | $80     |\n",
                "                    | Midwest    | $420    |\"\"\"\n",
                "\n",
                "    def transpose(self):\n",
                "        pd.read_csv(\n",
                "            StringIO(self.table) , \n",
                "            sep = \"|\" ,\n",
                "            skipinitialspace= True ,\n",
                "            header = 0 ,\n",
                "            index_col = False \n",
                "        )\n",
                "\n",
                "m = mellea.start_session()\n",
                "db = myDB()\n",
                "assert isinstance(db , MifiedProtocol)\n",
                "answer = m.query( db , \" What is the total sales of the east ?\")\n",
                "print(str(answer))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### How to find new updates in the code base that cause you to use older imports and throws errors ?\n",
                "```\n",
                "(mellea) ashoksharma@Ashoks-MacBook-Pro Mellea_IBM % cd mellea\n",
                "(mellea) ashoksharma@Ashoks-MacBook-Pro mellea % find . -name \"mify.py\"\n",
                "./mellea/stdlib/components/mify.py\n",
                "./docs/examples/mify/mify.py \n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Working with documents\n",
                "\n",
                "from mellea.stdlib.components.docs.richdocument import RichDocument\n",
                "# Note: This requires the docling package installed\n",
                "# rd = RichDocument.from_document_file(\"https://arxiv.org/pdf/1906.04043\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example of working with a table from a document\n",
                "from mellea.stdlib.components.docs.richdocument import Table\n",
                "# table1 : Table = rd.get_tables()[0] \n",
                "# print(table1.to_markdown())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Debugging: Session Reset & Last Turn\n",
                "\n",
                "These are the 3 things you should check *all the time* when learning:\n",
                "\n",
                "1. `m.last_prompt()` → what did you actually send?\n",
                "2. `m.ctx.last_turn()` → last user+assistant pair\n",
                "3. `m.ctx.view_for_generation()` (advanced) → what context will be sent next"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mellea.backends.types import ModelOption\n",
                "from mellea import start_session\n",
                "\n",
                "print(\"Initializing Mellea Session...\")\n",
                "m = mellea.MelleaSession(\n",
                "    backend=OllamaModelBackend(\n",
                "        model_id=\"granite4:micro\", \n",
                "        model_options={ ModelOption.SEED: 42, ModelOption.MAX_NEW_TOKENS:500 } # Fixed seed for reproducibility\n",
                "    )\n",
                ")\n",
                "print(\"Session started! Model: granite4:micro\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Debugging commands\n",
                "print(\"--- last_prompt ---\")\n",
                "print(m.last_prompt())\n",
                "\n",
                "print(\"\\n--- last_turn ---\")\n",
                "print(m.ctx.last_turn() if hasattr(m, \"ctx\") else \"no ctx\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Building Agents\n",
                "\n",
                "Mellea supports building agents like ReACT/Reasoning Loop agents. \n",
                "See `demo_agent.py` for a full implementation of a ReACT agent.\n",
                "\n",
                "## Interoperability\n",
                "\n",
                "- **MCP (Model Context Protocol)**: Expose Mellea tools to other AI clients. (See `demo_mcp.py`)\n",
                "- **crewAI Integration**: Mellea can be used as a backend for crewAI agents."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}