{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Welcome to Mellea (Docs: Welcome, Quickstart)\n",
                "\n",
                "Welcome to the interactive demo for **Mellea**, a library for writing generative programs. \n",
                "In this notebook, we will explore how Mellea allows you to write programs that generate content, validate it against requirements, and repair it if necessary.\n",
                "\n",
                "**Docs Reference:** [Welcome](https://docs.mellea.ai/overview/mellea-welcome), [Quickstart](https://docs.mellea.ai/overview/quick-start)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Initializing Mellea Session...\n",
                        "Session started! Model: granite4:micro\n"
                    ]
                }
            ],
            "source": [
                "import mellea\n",
                "from mellea.backends import ModelOption\n",
                "from mellea.backends.ollama import OllamaModelBackend\n",
                "\n",
                "# Initialize Mellea with the local Ollama backend and Granite model\n",
                "print(\"Initializing Mellea Session...\")\n",
                "m = mellea.MelleaSession(\n",
                "    backend=OllamaModelBackend(\n",
                "        model_id=\"granite4:micro\", \n",
                "        model_options={ModelOption.SEED: 42} # Fixed seed for reproducibility\n",
                "    )\n",
                ")\n",
                "print(\"Session started! Model: granite4:micro\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Your First Instruction\n",
                "\n",
                "The core of Mellea is the `instruct` method. You give it a natural language instruction, and it returns a generated result.\n",
                "Let's start with something simple."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/2 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;20m=== 16:34:05-INFO ======\n",
                        "SUCCESS\u001b[0m\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/2 [00:06<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Code in Python flows,\n",
                        "Elegance meets simplicity.\n",
                        "Logic takes its stage.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "response = m.instruct(\"Write a short haiku about writing python code.\")\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# The Loop: Instruct-Validate-Repair (Docs: Requirements)\n",
                "\n",
                "Generative models can be unpredictable. Mellea's power lies in its **Instruct-Validate-Repair** loop. \n",
                "You can define **Requirements** (`req`) and **Checks** (`check`) that the output *must* satisfy. If the model fails, Mellea uses a **Strategy** to try again or fix it.\n",
                "\n",
                "**Docs Reference:** [Requirements](https://docs.mellea.ai/overview/requirements)\n",
                "\n",
                "### Example: Generating a Helper Email with Constraints\n",
                "We want to generate an email that:\n",
                "1.  Has a salutation.\n",
                "2.  Does **not** use the word 'regards'.\n",
                "3.  (Optional strict check) Uses only lower-case letters (just to show validation failure and repair)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating email for Arush...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/5 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;20m=== 16:34:11-INFO ======\n",
                        "SUCCESS\u001b[0m\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/5 [00:05<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Success! Email generated:\n",
                        "Subject: Heartfelt Thanks for Your Contribution\n",
                        "\n",
                        "Dear Arush,\n",
                        "\n",
                        "I hope this message finds you well. I am writing to express my sincere gratitude for your invaluable contribution towards our recent team project.\n",
                        "\n",
                        "Your dedication, creativity and commitment did not go unnoticed, and they played an instrumental role in achieving the successful outcome of the project. The quality of work produced was truly impressive and a testament to your skills and abilities.\n",
                        "\n",
                        "Please allow me to reiterate how much we appreciate your hard work and all that you have contributed. Your efforts have been crucial in making this project a success.\n",
                        "\n",
                        "Thank you once again for everything.\n",
                        "\n",
                        "Best regards,\n",
                        "\n",
                        "[Your Name]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "from mellea.stdlib.requirements import req, check, simple_validate\n",
                "from mellea.stdlib.sampling import RejectionSamplingStrategy\n",
                "\n",
                "# Define requirements\n",
                "requirements = [\n",
                "    req(\"The email should have a salutation\"),\n",
                "    check(\"The email should not mention the word 'regards'\"),\n",
                "    # req(\"Use only lower-case letters\", validation_fn=simple_validate(lambda s: s.islower())) # Uncomment to test strict validation failure\n",
                "]\n",
                "\n",
                "def write_email(name, notes):\n",
                "    print(f\"Generating email for {name}...\")\n",
                "    email_candidate = m.instruct(\n",
                "        \"Write an email to {{name}} using the notes following: {{notes}}.\",\n",
                "        requirements=requirements,\n",
                "        # RejectionSamplingStrategy tries up to 'loop_budget' times to get a result that passes requirements\n",
                "        strategy=RejectionSamplingStrategy(loop_budget=5),\n",
                "        user_variables={\"name\": name, \"notes\": notes},\n",
                "        return_sampling_results=True,\n",
                "    )\n",
                "    \n",
                "    if email_candidate.success:\n",
                "        print(\"\\nSuccess! Email generated:\")\n",
                "        return str(email_candidate.result)\n",
                "    else:\n",
                "        print(\"\\nFailed to meet requirements.\")\n",
                "        return email_candidate.sampling_results[0].value\n",
                "\n",
                "# Test it\n",
                "print(write_email(\"Arush\", \"Thanks for helping with the team project.\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;20m=== 17:33:03-INFO ======\n",
                        "Starting Mellea session: backend=ollama, model=granite4:micro, context=SimpleContext\u001b[0m\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/5 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;20m=== 17:33:13-INFO ======\n",
                        "FAILED. Valid: 2/3\u001b[0m\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 20%|██        | 1/5 [00:09<00:38,  9.58s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;20m=== 17:33:19-INFO ======\n",
                        "SUCCESS\u001b[0m\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 20%|██        | 1/5 [00:15<01:03, 15.89s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "subject: recognition for your contributions\n",
                        "\n",
                        "dear arush,\n",
                        "\n",
                        "i hope this message finds you well. i am writing to express my gratitude for all that you contribute to our team environment. it has been truly inspiring to witness your dedication and commitment in every project we undertake.\n",
                        "\n",
                        "your willingness to help others and your active contribution to group projects have not gone unnoticed. these qualities make you an invaluable member of our team, and they highlight the importance of being a great team player.\n",
                        "\n",
                        "thank you once again for your hard work and positive attitude towards achieving our shared goals. keep up the fantastic work!\n",
                        "\n",
                        "warm regards,\n",
                        "\n",
                        "[your name]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "import mellea\n",
                "\n",
                "from mellea.stdlib.sampling import RejectionSamplingStrategy\n",
                "from mellea.stdlib.requirements import req,check,simple_validate\n",
                "\n",
                "# using a validation fn to validate your results as well as a check to validate your results\n",
                "requirements = [\n",
                "    req(\"The email should have a salutation\"),\n",
                "    req(\"Use only lower-case letters\", validation_fn=simple_validate(lambda s: s.islower())),\n",
                "    check(\"The email should not mention the word 'regards'\")\n",
                "        \n",
                "]\n",
                "\n",
                "def write_email(m:mellea.MelleaSession, names :str , notes:str)->str:\n",
                "    email_candidate = m.instruct(\n",
                "        \"Write an email to {{name}} using the notes following: {{notes}}.\",\n",
                "        requirements=requirements,\n",
                "        strategy = RejectionSamplingStrategy(loop_budget = 5),\n",
                "        user_variables={\"name\": names, \"notes\": notes},\n",
                "        return_sampling_results = True ,\n",
                "    )\n",
                "    if email_candidate.success :\n",
                "        return str(email_candidate.result)\n",
                "    else :\n",
                "        print(\"Expect sub-par output due to failure to meet requirements.\")\n",
                "        return email_candidate.sampling_results[0].value\n",
                "    \n",
                "m=mellea.start_session()\n",
                "print(write_email(m,\"Arush\",\"Arush has been a great team player, always willing to help others and contribute to group projects.\"))\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Advanced: Core Concepts & Generative Programming\n",
                "\n",
                "**Docs Reference:** [Generative Programming](https://docs.mellea.ai/overview/project-mellea), [Core Concepts](https://docs.mellea.ai/core-concept/generative-slots)\n",
                "\n",
                "In this section, we touch on more advanced concepts. Mellea treats prompts not just as strings, but as programs with **Slots**, **Context**, and **Agents**.\n",
                "\n",
                "### Variable Injection (Context)\n",
                "You saw `user_variables` above. This is part of Mellea's Context Management system, allowing you to inject data safely into prompts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/2 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;20m=== 16:34:11-INFO ======\n",
                        "SUCCESS\u001b[0m\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/2 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Hello in Spanish: The translation of \"Hello\" in Spanish is \"Hola\".\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Quick example of dynamic variables\n",
                "response = m.instruct(\n",
                "    \"Translate the following word to Spanish: {{word}}\",\n",
                "    user_variables={\"word\": \"Hello\"}\n",
                ")\n",
                "print(f\"Hello in Spanish: {response}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/2 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;20m=== 16:34:18-INFO ======\n",
                        "SUCCESS\u001b[0m\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/2 [00:06<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sure! Imagine you have a big box of toys. Now, when you want to play with your favorite toy car, in the regular games or world (which we can call 'classical computing'), it's like saying \"Okay, let's only look for this car right now\". You don't find other cars, you just search one by one until you find it.\n",
                        "\n",
                        "Now, quantum computing is more like having a magic box of toys. In this magical box, your toy car and all the other cars can be in many places at once! It's kind of like your car being both here and there at the same time. This makes it much faster to find what you want because everything isn't stuck just in one place.\n",
                        "\n",
                        "So, quantum computing is a bit different from our regular games or world - it's more magical and can do things really fast that we can't do otherwise!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Enter your instruction below\n",
                "my_instruction = \"Explain quantum computing to a 5 year old.\"\n",
                "\n",
                "result = m.instruct(my_instruction)\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/2 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;20m=== 17:33:51-INFO ======\n",
                        "SUCCESS\u001b[0m\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/2 [00:04<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The mathematical expression \"2x2\" represents the multiplication of two numbers: 2 and 2. Therefore, 2 multiplied by 2 equals 4. In other words, if you have two groups with two items in each group, the total number of items would be four.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# using model options\n",
                "import mellea\n",
                "from mellea.backends import ModelOption  #its mellea.backends\n",
                "from mellea.backends.ollama import OllamaModelBackend\n",
                "from mellea.backends import model_ids\n",
                "\n",
                "m = mellea.MelleaSession(\n",
                "    backend=OllamaModelBackend(\n",
                "        model_id=\"granite4:micro\",model_options={ModelOption.SEED: 42}\n",
                "    )\n",
                ")\n",
                "\n",
                "answer = m.instruct(\n",
                "    \"What is 2x2?\",\n",
                "    model_options={\n",
                "        \"temperature\": 0.1,\n",
                "    },\n",
                ")\n",
                "\n",
                "print(str(answer))\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generative Slots\n",
                "\n",
                "``GenerativeSlot`` is a function whose implementation is provieded by an LLM.In Mellea you define these using ``@generative`` decorator\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;20m=== 17:13:13-INFO ======\n",
                        "Starting Mellea session: backend=ollama, model=granite4:micro, context=SimpleContext\u001b[0m\n",
                        "positive\n"
                    ]
                }
            ],
            "source": [
                "from typing import Literal\n",
                "from mellea import generative, start_session\n",
                "\n",
                "\n",
                "@generative\n",
                "def classify_sentiment(text: str) -> Literal[\"positive\", \"negative\", \"neutral\"]:\n",
                "    \"\"\"Classify the sentiment of the input text as 'positive', 'negative', or 'neutral'.\"\"\"\n",
                "    ...\n",
                " \n",
                "m = start_session()\n",
                "sentiment = classify_sentiment(m, text = \"I love this product!\")\n",
                "print(sentiment)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;20m=== 17:31:27-INFO ======\n",
                        "Starting Mellea session: backend=ollama, model=granite4:micro, context=ChatContext, model_options={'@@@max_new_tokens@@@': 10}\u001b[0m\n",
                        "grade =  9\n",
                        "grade =  2\n",
                        "grade =  7\n"
                    ]
                }
            ],
            "source": [
                "# from mellea.std.base import ChatContext is old \n",
                "from mellea.stdlib.context import ChatContext\n",
                "from mellea import start_session\n",
                "from mellea.backends import ModelOption\n",
                "from mellea import generative   \n",
                "\n",
                "# ChatContext is used to maintain conversation history across multiple model calls. \n",
                "# Unlike SimpleContext (the default), which resets the chat history on each call, \n",
                "# ChatContext behaves like a chat history where previous messages are remembered.\n",
                "\n",
                "@generative\n",
                "def grade_syntax(code: str) -> int:\n",
                "    \"\"\" Grade the code based on correct implementation of the function\n",
                "    args:\n",
                "        code: str (to be graded)\n",
                "    returns:\n",
                "        int : a grade between 1(worst) and 10(best)\n",
                "    \"\"\"\n",
                "codes = (\n",
                "    \" def add(a, b):\\n    return a + b\",\n",
                "    \"def subtract(a,b): cout<<a-b<<endl\",\n",
                "    \"int multiply(int a,int b) { return a*b}\"\n",
                ")\n",
                "m = start_session(ctx = ChatContext() , model_options = {ModelOption.MAX_NEW_TOKENS: 10})\n",
                "for code in codes :\n",
                "    grade = grade_syntax(m, code = code)\n",
                "    print(\"grade = \",grade)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Advanced: Using Generative slots to Provide Compositionality Across Module Boundaries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "# file: https://github.com/generative-computing/mellea/blob/main/docs/examples/tutorial/compositionality_with_generative_slots.py#L1-L18\n",
                "from mellea import generative\n",
                "from typing import Literal\n",
                "\n",
                "# The Summarizer Library\n",
                "@generative\n",
                "def summarize_meeting(transcript: str) -> str:\n",
                "  \"\"\"Summarize the meeting transcript into a concise paragraph of main points.\"\"\"\n",
                "\n",
                "@generative\n",
                "def summarize_contract(contract_text: str) -> str:\n",
                "  \"\"\"Produce a natural language summary of contract obligations and risks.\"\"\"\n",
                "\n",
                "@generative\n",
                "def summarize_short_story(story: str) -> str:\n",
                "  \"\"\"Summarize a short story, with one paragraph on plot and one paragraph on broad themes.\"\"\"\n",
                "\n",
                "\n",
                "# The Decision Aides Library\n",
                "@generative\n",
                "def propose_business_decision(summary: str) -> str:\n",
                "  \"\"\"Given a structured summary with clear recommendations, propose a business decision.\"\"\"\n",
                "\n",
                "@generative\n",
                "def generate_risk_mitigation(summary: str) -> str:\n",
                "  \"\"\"If the summary contains risk elements, propose mitigation strategies.\"\"\"\n",
                "\n",
                "@generative\n",
                "def generate_novel_recommendations(summary: str) -> str:\n",
                "  \"\"\"Provide a list of novel recommendations that are similar in plot or theme to the short story summary.\"\"\"\n",
                "\n",
                "# Compose the libraries.\n",
                "@generative\n",
                "def has_structured_conclusion(summary: str) -> Literal[\"yes\", \"no\"]:\n",
                "  \"\"\"Determine whether the summary contains a clearly marked conclusion or recommendation.\"\"\"\n",
                "\n",
                "@generative\n",
                "def contains_actionable_risks(summary: str) -> Literal[\"yes\", \"no\"]:\n",
                "  \"\"\"Check whether the summary contains references to business risks or exposure.\"\"\"\n",
                "\n",
                "@generative\n",
                "def has_theme_and_plot(summary: str) -> Literal[\"yes\", \"no\"]:\n",
                "  \"\"\"Check whether the summary contains both a plot and thematic elements.\"\"\"\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;20m=== 14:47:18-INFO ======\n",
                        "Starting Mellea session: backend=ollama, model=granite4:micro, context=SimpleContext\u001b[0m\n",
                        "Mitigation: Based on the summary provided, the following risk mitigation strategies are proposed:\n",
                        "\n",
                        "1. **Supplier Diversification**: Explore alternative suppliers for micro-carburetors to mitigate potential price increases due to tariffs. This could involve conducting market research and possibly establishing new supplier relationships.\n",
                        "\n",
                        "2. **Market Analysis and Forecasting**: Forecast erosion of the industrial segment sales due to increased adoption of self-interlocking leafscrews. Understanding customer trends can help in strategizing product development and marketing approaches.\n",
                        "\n",
                        "3. **Product Development Assessment**: Assess feasibility of reviving manual seal production, which could cater to clients seeking authentic, manual fasteners in specific markets. This could involve a cost-benefit analysis and feasibility study.\n",
                        "\n",
                        "4. **Scenario Planning for Cost Pass-Through or Feature-Based Differentiation**: Plan scenarios on how to pass through the increased BOM costs or differentiate based on features to maintain profitability while responding to market demands.\n",
                        "\n",
                        "Decision: The proposed business decision based on the given summary is to explore alternative suppliers for micro-carburetors and assess the feasibility of reviving manual seal production. Additionally, forecasting erosion of the industrial market due to leafscrews and scenario-planning cost pass-through or feature-based differentiation are recommended.\n"
                    ]
                }
            ],
            "source": [
                "m = start_session()\n",
                "transcript = \"\"\"Meeting Transcript: Market Risk Review -- Self-Sealing Stembolts Division\n",
                "Date: December 1, 3125\n",
                "Attendees:\n",
                "\n",
                "Karen Rojas, VP of Product Strategy\n",
                "\n",
                "Derek Madsen, Director of Global Procurement\n",
                "\n",
                "Felicia Zheng, Head of Market Research\n",
                "\n",
                "Tom Vega, CFO\n",
                "\n",
                "Luis Tran, Engineering Liaison\n",
                "\n",
                "Karen Rojas:\n",
                "Thanks, everyone, for making time on short notice. As you've all seen, we've got three converging market risks we need to address: tariffs on micro-carburetors, increased adoption of the self-interlocking leafscrew, and, believe it or not, the \"hipsterfication\" of the construction industry. I need all on deck and let's not waste time. Derek, start.\n",
                "\n",
                "Derek Madsen:\n",
                "Right. As of Monday, the 25% tariff on micro-carburetors sourced from the Pan-Alpha Centauri confederacy is active. We tried to pre-purchase a three-month buffer, but after that, our unit cost rises by $1.72. That's a 9% increase in the BOM cost of our core model 440 stembolt. Unless we find alternative suppliers or pass on the cost, we're eating into our already narrow margin.\n",
                "\n",
                "Tom Vega:\n",
                "We cannot absorb that without consequences. If we pass the cost downstream, we risk losing key mid-tier OEM clients. And with the market already sniffing around leafscrew alternatives, this makes us more vulnerable.\n",
                "\n",
                "Karen:\n",
                "Lets pause there. Felicia, give us the quick-and-dirty on the leafscrew.\n",
                "\n",
                "Felicia Zheng:\n",
                "It's ugly. Sales of the self-interlocking leafscrew—particularly in modular and prefab construction—are up 38% year-over-year. It's not quite a full substitute for our self-sealing stembolts, but they are close enough in function that some contractors are making the switch. Their appeal? No micro-carburetors, lower unit complexity, and easier training for install crews. We estimate we've lost about 12% of our industrial segment to the switch in the last two quarters.\n",
                "\n",
                "Karen:\n",
                "Engineering, Luis; your take on how real that risk is?\n",
                "\n",
                "Luis Tran:\n",
                "Technically, leafscrews are not as robust under high-vibration loads. But here's the thing: most of the modular prefab sites don not need that level of tolerance. If the design spec calls for durability over 10 years, we win. But for projects looking to move fast and hit 5-year lifespans? The leafscrew wins on simplicity and cost.\n",
                "\n",
                "Tom:\n",
                "So they're eating into our low-end. That's our volume base.\n",
                "\n",
                "Karen:\n",
                "Exactly. Now let's talk about this last one: the “hipsterfication” of construction. Felicia?\n",
                "\n",
                "Felicia:\n",
                "So this is wild. We're seeing a cultural shift in boutique and residential construction—especially in markets like Beckley, West Sullivan, parts of Osborne County, where clients are requesting \"authentic\" manual fasteners. They want hand-sealed bolts, visible threads, even mismatched patinas. It's an aesthetic thing. Function is almost secondary. Our old manual-seal line from the 3180s? People are hunting them down on auction sites.\n",
                "\n",
                "Tom:\n",
                "Well, I'm glad I don't have to live in the big cities... nothing like this would ever happen in downt-to-earth places Brooklyn, Portland, or Austin.\n",
                "\n",
                "Luis:\n",
                "We literally got a request from a design-build firm in Keough asking if we had any bolts “pre-distressed.”\n",
                "\n",
                "Karen:\n",
                "Can we spin this?\n",
                "\n",
                "Tom:\n",
                "If we keep our vintage tooling and market it right, maybe. But that's niche. It won't offset losses in industrial and prefab.\n",
                "\n",
                "Karen:\n",
                "Not yet. But we may need to reframe it as a prestige line—low volume, high margin. Okay, action items. Derek, map alternative micro-carburetor sources. Felicia, get me a forecast on leafscrew erosion by sector. Luis, feasibility of reviving manual seal production. Tom, let's scenario-plan cost pass-through vs. feature-based differentiation.\n",
                "\n",
                "Let's reconvene next week with hard numbers. Thanks, all.\"\"\"\n",
                "summary = summarize_meeting(m, transcript=transcript)\n",
                "\n",
                "if contains_actionable_risks(m, summary=summary) == \"yes\":\n",
                "    mitigation = generate_risk_mitigation(m, summary=summary)\n",
                "    print(f\"Mitigation: {mitigation}\")\n",
                "else:\n",
                "    print(\"Summary does not contain actionable risks.\")\n",
                "if has_structured_conclusion(m, summary=summary) == \"yes\":\n",
                "    decision = propose_business_decision(m, summary=summary)\n",
                "    print(f\"Decision: {decision}\")\n",
                "else:\n",
                "    print(\"Summary lacks a structured conclusion.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Mify Objects\n",
                "\n",
                "```MObject``` storing data along side with its relevant operations(tools).This allows LLMs to interact with both tthe data and methods in a unifiied structured manner , simplifies exposinhg only specific fields and methods for LLMs to interact with.\n",
                "\n",
                "```Mobject``` pattern also provides a way of evolvoing exsisting classical codebases into generative programs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[38;20m=== 16:15:47-INFO ======\n",
                        "Starting Mellea session: backend=ollama, model=granite4:micro, context=SimpleContext\u001b[0m\n",
                        "The total sales for the East region, which includes both the Northeast and Southeast, would be:\n",
                        "\n",
                        "Northeast: $250\n",
                        "Southeast: $80\n",
                        "\n",
                        "Total Sales = $250 + $80 = $330. \n",
                        "\n",
                        "So, the total sales for the East are $330.\n"
                    ]
                }
            ],
            "source": [
                "import mellea\n",
                "# old was mellea.stdlib.mify \n",
                "from mellea.stdlib.components.mify import mify, MifiedProtocol\n",
                "import pandas as pd\n",
                "from io import StringIO\n",
                "\n",
                "@mify(fields_include = {\"table\"} , template = \"{{table}}\")\n",
                "class myDB:\n",
                "    table : str = \"\"\"| Store      | Sales   |\n",
                "                    | ---------- | ------- |\n",
                "                    | Northeast  | $250    |\n",
                "                    | Southeast  | $80     |\n",
                "                    | Midwest    | $420    |\"\"\"\n",
                "\n",
                "    def transpose(self):\n",
                "        pd.read_csv(\n",
                "            StringIO(self.table) , \n",
                "            sep = \"|\" ,\n",
                "            skipinitialspace= True ,\n",
                "            header = 0 ,\n",
                "            index_col = False \n",
                "        )\n",
                "\n",
                "m = mellea.start_session()\n",
                "db = myDB()\n",
                "assert isinstance(db , MifiedProtocol)\n",
                "answer = m.query( db , \" What is the total sales of the east ?\")\n",
                "print(str(answer))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### How to find new updates in the code base that cause you to use older imports and throws errors ?\n",
                "```\n",
                "(mellea) ashoksharma@Ashoks-MacBook-Pro Mellea_IBM % cd mellea\n",
                "(mellea) ashoksharma@Ashoks-MacBook-Pro mellea % find . -name \"mify.py\"\n",
                "./mellea/stdlib/components/mify.py\n",
                "./docs/examples/mify/mify.py \n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Working with documents\n",
                "\n",
                "from mellea.stdlib.components.docs.richdocument import RichDocument\n",
                "rd = RichDocument.from_document_file(\"https://arxiv.org/pdf/1906.04043\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Table 1: Cross-validated results of fake-text discriminators. Distributional information yield a higher informativeness than word-features in a logistic regression.\n",
                        "\n",
                        "| Feature                              | AUC         |\n",
                        "|--------------------------------------|-------------|\n",
                        "| Bag of Words                         | 0.63 ± 0.11 |\n",
                        "| (Test 1 - GPT-2) Average Probability | 0.71 ± 0.25 |\n",
                        "| (Test 2 - GPT-2) Top-K Buckets       | 0.87 ± 0.07 |\n",
                        "| (Test 1 - BERT) Average Probability  | 0.70 ± 0.27 |\n",
                        "| (Test 2 - BERT) Top-K Buckets        | 0.85 ± 0.09 |\n"
                    ]
                }
            ],
            "source": [
                "from mellea.stdlib.components.docs.richdocument import Table\n",
                "table1 : Table = rd.get_tables()[0] \n",
                "print(table1.to_markdown())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mellea.backends.types import ModelOption\n",
                "from mellea import start_session\n",
                "\n",
                "print(\"Initializing Mellea Session...\")\n",
                "m = mellea.MelleaSession(\n",
                "    backend=OllamaModelBackend(\n",
                "        model_id=\"granite4:micro\", \n",
                "        model_options={ ModelOption.SEED: 42, ModelOption.MAX_NEW_TOKENS:500 } # Fixed seed for reproducibility\n",
                "    )\n",
                ")\n",
                "print(\"Session started! Model: granite4:micro\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (Mellea Project)",
            "language": "python",
            "name": "mellea_project"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
